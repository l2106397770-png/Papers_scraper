"""
å­¦æœ¯ä¼šè®®è®ºæ–‡çˆ¬å–å·¥å…·
ç½‘å€ï¼šhttps://papers.cool
åŠŸèƒ½ï¼šçˆ¬å– papers.cool ç½‘ç«™çš„æŒ‡å®šä¼šè®®è®ºæ–‡ä¿¡æ¯ï¼Œæ”¯æŒå¯¼å‡ºä¸º CSV æ–‡ä»¶
æ”¯æŒä¼šè®®ï¼šNeurIPSã€ACLã€AAAIã€EMNLPã€NAACLã€ICMLã€CVPRã€ICLR ç­‰
"""

import argparse
import re
import csv
from typing import Optional, Dict, Any, Union, List

import requests
from bs4 import BeautifulSoup  # ä¿ç•™å¯¼å…¥ï¼ˆå¦‚éœ€æ‰©å±•DOMè§£æå¯ç›´æ¥ä½¿ç”¨ï¼‰
from lxml import etree
from requests.exceptions import (
    RequestException,
    ConnectTimeout,
    ReadTimeout,
    SSLError
)
from config import (
    SUPPORTED_VENUES,
    DEFAULT_USER_AGENT,
    DEFAULT_TIMEOUT,
    DEFAULT_CSV_PATH,
    MAX_PAPER_COUNT,
    PAPER_FIELDS
)


def parse_papers_info(html_str: str) -> List[Dict[str, str]]:
    """
    è§£æHTMLå­—ç¬¦ä¸²ä¸­çš„è®ºæ–‡ä¿¡æ¯
    Args:
        html_str: åŒ…å«è®ºæ–‡åˆ—è¡¨çš„HTMLå­—ç¬¦ä¸²
    Returns:
        è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«è®ºæ–‡è¯¦æƒ…çš„å­—å…¸
    """
    # ç”¨lxmlè§£æHTMLï¼ˆè‡ªåŠ¨ä¿®å¤ä¸è§„èŒƒæ ‡ç­¾ï¼‰
    html = etree.HTML(html_str)

    # æå–è®ºæ–‡æ€»æ•°
    total_text_list = html.xpath('//*[@id="venue"]/p/text()')
    total_count = "æœªçŸ¥"
    if total_text_list:
        total_text = total_text_list[-1]
        pattern = re.compile(r'(\d+)')
        match_result = pattern.findall(total_text)
        if match_result:
            total_count = match_result[0]
    print(f"ğŸ“Š æ­¤æ¿å—å…± {total_count} ç¯‡è®ºæ–‡")

    # å®šä½æ ¸å¿ƒè®ºæ–‡åˆ—è¡¨å®¹å™¨ï¼ˆclass="papers"ï¼‰
    papers_elements = html.xpath('//*[@class="papers"]')
    if not papers_elements:
        print("âš ï¸  æœªæ‰¾åˆ°class='papers'çš„è®ºæ–‡åˆ—è¡¨å®¹å™¨")
        return []

    # è§£ææ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    target_element = papers_elements[0]
    papers_list = []

    for tag in target_element:
        paper_info = {field: "" for field in PAPER_FIELDS if field != "ID"}

        # å…³é”®è¯ï¼šå¤–å±‚divçš„keywordså±æ€§
        keywords = tag.xpath('./@keywords')
        if keywords:
            paper_info["Keywords"] = keywords[0].strip()

        # æ ‡é¢˜ï¼šh2.titleä¸‹çš„aæ ‡ç­¾æ–‡æœ¬
        title_list = tag.xpath('.//h2[@class="title"]/a/text()')
        if title_list:
            paper_info["Title"] = title_list[0].strip()

        # ä½œè€…ï¼šp.metainfo.authorsä¸‹æ‰€æœ‰a.authorçš„æ–‡æœ¬ï¼ˆé€—å·æ‹¼æ¥ï¼‰
        authors_list = tag.xpath('.//p[@class="metainfo authors notranslate"]/a/text()')
        if authors_list:
            paper_info["Authors"] = ", ".join(author.strip() for author in authors_list)

        # æ‘˜è¦ï¼šp.summaryçš„æ–‡æœ¬
        abstract_list = tag.xpath('.//p[contains(@class, "summary")]/text()')
        if abstract_list:
            paper_info["Abstract"] = abstract_list[0].strip()

        # PDFé“¾æ¥ï¼ša.title-pdfçš„dataå±æ€§
        pdf_link_list = tag.xpath('.//h2[@class="title"]/a[@class="title-pdf notranslate"]/@data')
        if pdf_link_list:
            paper_info["PDF_Link"] = pdf_link_list[0].strip()

        # è®ºæ–‡ç±»å‹ï¼šp.metainfo.subjectsçš„æ–‡æœ¬
        type_list = tag.xpath('.//p[@class="metainfo subjects"]/a/text()')
        if type_list:
            paper_info["Type"] = type_list[0].strip()

        # è¿‡æ»¤ç©ºæ•°æ®ï¼ˆæ ‡é¢˜ä¸ºç©ºåˆ™è·³è¿‡ï¼‰
        if paper_info["Title"]:
            papers_list.append(paper_info)

    return papers_list


def crawl_website(
    url: str,
    method: str = "GET",
    params: Optional[Dict[str, Any]] = None,
    data: Optional[Union[Dict[str, Any], str]] = None,
    headers: Optional[Dict[str, str]] = None,
    cookies: Optional[Dict[str, str]] = None,
    proxies: Optional[Dict[str, str]] = None,
    timeout: float = DEFAULT_TIMEOUT,
    verify_ssl: bool = True,
    allow_redirects: bool = True,
    encoding: Optional[str] = None
) -> Dict[str, Any]:
    """
    é€šç”¨ç½‘ç«™çˆ¬å–å‡½æ•°ï¼Œæ”¯æŒGET/POSTè¯·æ±‚ï¼ŒåŒ…å«å®Œå–„çš„å¼‚å¸¸å¤„ç†

    Args:
        url: ç›®æ ‡ç½‘ç«™URL
        method: è¯·æ±‚æ–¹æ³•ï¼ˆGET/POSTï¼‰ï¼Œé»˜è®¤GET
        params: GETè¯·æ±‚æŸ¥è¯¢å‚æ•°ï¼ˆå­—å…¸ï¼‰
        data: POSTè¯·æ±‚æ•°æ®ï¼ˆå­—å…¸/å­—ç¬¦ä¸²ï¼‰
        headers: è¯·æ±‚å¤´å­—å…¸ï¼ˆé»˜è®¤æ·»åŠ åŸºç¡€User-Agentï¼‰
        cookies: Cookieå­—å…¸ï¼ˆç»´æŒç™»å½•çŠ¶æ€ç­‰ï¼‰
        proxies: ä»£ç†é…ç½®å­—å…¸ï¼ˆæ ¼å¼ï¼š{"http": "http://IP:ç«¯å£", "https": "..."}ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        verify_ssl: æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼Œé»˜è®¤True
        allow_redirects: æ˜¯å¦å…è®¸é‡å®šå‘ï¼Œé»˜è®¤True
        encoding: å“åº”ç¼–ç ï¼ˆé»˜è®¤è‡ªåŠ¨è¯†åˆ«ï¼‰

    Returns:
        çˆ¬å–ç»“æœå­—å…¸ï¼š
        - success: çˆ¬å–æ˜¯å¦æˆåŠŸï¼ˆboolï¼‰
        - status_code: å“åº”çŠ¶æ€ç ï¼ˆNoneè¡¨ç¤ºå¤±è´¥ï¼‰
        - content: å“åº”æ–‡æœ¬ï¼ˆå¤±è´¥æ—¶ä¸ºé”™è¯¯ä¿¡æ¯ï¼‰
        - headers: å“åº”å¤´ï¼ˆå­—å…¸ï¼Œå¤±è´¥æ—¶ä¸ºNoneï¼‰
        - encoding: å®é™…ä½¿ç”¨çš„ç¼–ç ï¼ˆå¤±è´¥æ—¶ä¸ºNoneï¼‰
    """
    # åˆå¹¶é»˜è®¤è¯·æ±‚å¤´ä¸ç”¨æˆ·è‡ªå®šä¹‰è¯·æ±‚å¤´
    final_headers = {
        "User-Agent": DEFAULT_USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": url
    }
    if headers:
        final_headers.update(headers)

    # åˆå§‹åŒ–è¿”å›ç»“æœ
    result = {
        "success": False,
        "status_code": None,
        "content": "",
        "headers": None,
        "encoding": None
    }

    try:
        # å‘é€HTTPè¯·æ±‚
        response = requests.request(
            method=method.upper(),
            url=url,
            params=params,
            data=data,
            headers=final_headers,
            cookies=cookies,
            proxies=proxies,
            timeout=timeout,
            verify=verify_ssl,
            allow_redirects=allow_redirects
        )

        # éªŒè¯å“åº”çŠ¶æ€ï¼ˆ2xxä¸ºæˆåŠŸï¼‰
        response.raise_for_status()

        # å¤„ç†ç¼–ç ï¼ˆä¼˜å…ˆç”¨æˆ·æŒ‡å®šï¼Œå…¶æ¬¡è‡ªåŠ¨è¯†åˆ«ï¼‰
        if encoding:
            response.encoding = encoding
        else:
            response.encoding = response.apparent_encoding or response.encoding

        # å¡«å……æˆåŠŸç»“æœ
        result.update({
            "success": True,
            "status_code": response.status_code,
            "content": response.text,
            "headers": dict(response.headers),
            "encoding": response.encoding
        })

    except ConnectTimeout:
        result["content"] = f"è¿æ¥è¶…æ—¶ï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰"
    except ReadTimeout:
        result["content"] = f"è¯»å–è¶…æ—¶ï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰"
    except SSLError:
        result["content"] = "SSLè¯ä¹¦éªŒè¯å¤±è´¥ï¼ˆå¯å°è¯•æ·»åŠ  --verify-ssl=False å‚æ•°ï¼‰"
    except RequestException as e:
        # æ•è·requestsç›¸å…³å¼‚å¸¸ï¼ˆ4xx/5xxçŠ¶æ€ç ã€URLé”™è¯¯ç­‰ï¼‰
        result["content"] = f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
        if hasattr(e, 'response') and e.response is not None:
            result["status_code"] = e.response.status_code
    except Exception as e:
        # æ•è·å…¶ä»–æœªé¢„æœŸå¼‚å¸¸
        result["content"] = f"æœªçŸ¥é”™è¯¯ï¼š{str(e)}"

    return result


def save_papers_to_csv(paper_list: List[Dict[str, str]], csv_save_path: str = DEFAULT_CSV_PATH) -> None:
    """
    å°†è®ºæ–‡ä¿¡æ¯ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆè‡ªåŠ¨æ·»åŠ å”¯ä¸€IDï¼‰
    Args:
        paper_list: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼ˆparse_papers_infoçš„è¿”å›å€¼ï¼‰
        csv_save_path: CSVæ–‡ä»¶ä¿å­˜è·¯å¾„
    """
    if not paper_list:
        print("âš ï¸  æ— æœ‰æ•ˆè®ºæ–‡æ•°æ®ï¼Œè·³è¿‡CSVä¿å­˜")
        return

    # ä¸ºæ¯ç¯‡è®ºæ–‡æ·»åŠ å”¯ä¸€IDï¼ˆä»1å¼€å§‹é€’å¢ï¼‰
    papers_with_id = []
    for idx, paper in enumerate(paper_list, 1):
        paper_with_id = paper.copy()
        paper_with_id["ID"] = str(idx)
        papers_with_id.append(paper_with_id)

    # å†™å…¥CSVæ–‡ä»¶
    try:
        with open(csv_save_path, "w", newline="", encoding="utf-8-sig") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=PAPER_FIELDS)
            writer.writeheader()
            writer.writerows(papers_with_id)

        print(f"âœ… æˆåŠŸä¿å­˜ {len(papers_with_id)} ç¯‡è®ºæ–‡åˆ°ï¼š{csv_save_path}")
    except PermissionError:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥ï¼šæ— å†™å…¥æƒé™ï¼ˆè·¯å¾„ï¼š{csv_save_path}ï¼‰")
    except FileNotFoundError:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥ï¼šè·¯å¾„ä¸å­˜åœ¨ï¼ˆè·¯å¾„ï¼š{csv_save_path}ï¼‰")
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥ï¼š{str(e)}")


def validate_args(args: argparse.Namespace) -> tuple[bool, str, int]:
    """
    éªŒè¯å‘½ä»¤è¡Œå‚æ•°æœ‰æ•ˆæ€§
    Args:
        args: è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
    Returns:
        (æ˜¯å¦æœ‰æ•ˆ, ä¼šè®®ç±»å‹, è®ºæ–‡æ•°é‡)
    """
    # éªŒè¯ä¼šè®®ç±»å‹
    if args.venue_type not in SUPPORTED_VENUES:
        print(f"âŒ ä¸æ”¯æŒçš„ä¼šè®®ç±»å‹ï¼š{args.venue_type}")
        print(f"ğŸ“‹ æ”¯æŒçš„ä¼šè®®ç±»å‹ï¼š{', '.join(SUPPORTED_VENUES[:10])}...ï¼ˆå…±{len(SUPPORTED_VENUES)}ä¸ªï¼‰")
        return False, "", 0

    # éªŒè¯è®ºæ–‡æ•°é‡
    if args.count == "all":
        paper_count = MAX_PAPER_COUNT
    else:
        try:
            paper_count = int(args.count)
            if paper_count <= 0:
                print(f"âŒ è®ºæ–‡æ•°é‡å¿…é¡»ä¸ºæ­£æ•´æ•°ï¼ˆå½“å‰ï¼š{args.count}ï¼‰")
                return False, "", 0
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„è®ºæ–‡æ•°é‡ï¼š{args.count}ï¼ˆæ”¯æŒï¼šæ­£æ•´æ•°æˆ–'all'ï¼‰")
            return False, "", 0

    return True, args.venue_type, paper_count


def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‚æ•° â†’ çˆ¬å– â†’ è§£æ â†’ ä¿å­˜"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ğŸ“š å­¦æœ¯ä¼šè®®è®ºæ–‡çˆ¬å–å·¥å…·ï¼ˆæ”¯æŒ papers.coolï¼‰')
    
    parser.add_argument(
        '--venue_type',
        type=str,
        default='ACL.2025',
        help=f'ä¼šè®®ç±»å‹ï¼ˆé»˜è®¤ï¼šACL.2025ï¼Œæ”¯æŒ{len(SUPPORTED_VENUES)}ä¸ªä¼šè®®ï¼‰'
    )
    
    parser.add_argument(
        '--count',
        type=str,
        default='all',
        help='æŠ“å–è®ºæ–‡æ•°é‡ï¼ˆé»˜è®¤ï¼šallï¼Œæ”¯æŒæ­£æ•´æ•°å¦‚50/100ï¼‰'
    )
    
    parser.add_argument(
        '--save_path',
        type=str,
        default=DEFAULT_CSV_PATH,
        help=f'CSVä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ï¼š{DEFAULT_CSV_PATH}ï¼‰'
    )
    
    parser.add_argument(
        '--verify_ssl',
        type=bool,
        default=True,
        help='æ˜¯å¦éªŒè¯SSLè¯ä¹¦ï¼ˆé»˜è®¤ï¼šTrueï¼ŒHTTPSæŠ¥é”™æ—¶å¯è®¾ä¸ºFalseï¼‰'
    )
    
    args = parser.parse_args()

    # éªŒè¯å‚æ•°æœ‰æ•ˆæ€§
    valid, venue_type, paper_count = validate_args(args)
    if not valid:
        return

    # æ„å»ºçˆ¬å–URL
    url = f"https://papers.cool/venue/{venue_type}?show={paper_count}"
    print(f"ğŸš€ å¼€å§‹çˆ¬å–ï¼š{url}")

    # æ‰§è¡Œçˆ¬å–
    crawl_result = crawl_website(url=url, verify_ssl=args.verify_ssl)
    if not crawl_result["success"]:
        print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{crawl_result['content']}")
        return
    print(f"âœ… çˆ¬å–æˆåŠŸï¼çŠ¶æ€ç ï¼š{crawl_result['status_code']}")

    # è§£æè®ºæ–‡ä¿¡æ¯
    print("ğŸ” å¼€å§‹è§£æè®ºæ–‡ä¿¡æ¯...")
    papers = parse_papers_info(crawl_result["content"])
    print(f"âœ… è§£æå®Œæˆï¼å…±è·å– {len(papers)} ç¯‡æœ‰æ•ˆè®ºæ–‡")

    # ä¿å­˜åˆ°CSV
    save_papers_to_csv(papers, csv_save_path=args.save_path)
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()